{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks with Tensorflow\n",
    "## Overview\n",
    "\n",
    "### What You'll Learn\n",
    "In this section, you'll learn how to implement a neural network with tensorflow.\n"
    "\n",
    "### Prerequisites\n",
    "Before starting this section, you should have an understanding of\n",
    "1. [scikit-learn](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/intro_ml_scikit.ipynb)\n",
    "2. [Basic Python (functions, loops, lists)](https://github.com/HackBinghamton/PythonWorkshop)\n",
    "3. Intro to Neural Networks ADD LINK\n",
    "4. (Optional) [Matplotlib and numpy](https://github.com/HackBinghamton/DataScienceWorkshop)\n",
    "\n",
    "### Initial Setup Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST Dataset\n",
    "We were able to achieve a respectable accuracy on scikit-learn's `digits` dataset using logistic regression. However,\n",
    "`digits` is quite small and simple - each image was only 8 pixels by 8 pixels. Modern computer monitors display millions of pixels at a time, so an 8 x 8 image is primitive compared to real-world data we'd be working with.\n",
    "\n",
    "The MNIST dataset, on the other hand, consists of 28 x 28 pixel images of handwritten digits for classification. This is closer to a real-world dataset, but the increased complexity of the images means that our logistic regression model from Part 1 will be noticably less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the MNIST Dataset\n",
    "First, we'll import tensorflow and load the MNIST dataset into our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load in the MNIST dataset from TensorFlow\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to split our data into training and testing sets. Unlike the `digits` dataset, the MNIST dataset is already split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's quickly familiarize ourselves with the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "print(\"The shape of the training set's data array is\", X_train.shape)\n",
    "print(\"The shape of the training set's target array is\", Y_train.shape)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"The shape of the testing set's data array is\", X_test.shape)\n",
    "print(\"The shape of the testing set's target array is\", Y_test.shape)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"The minimum pixel value in the training set's data array is\", X_test.min())\n",
    "print(\"The maximum pixel value in the training set's data array is\", X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel value in the MNIST dataset is between 0 and 255. However, most machine learning algorithms work best when dealing with values between 0 and 1, inclusive. This is because many machine learning algorithms use [Euclidian Distance](https://en.wikipedia.org/wiki/Euclidean_distance) when comparing two data points.\n",
    "\n",
    "To keep our values between 0 and 1, we will implement [unit vector scaling](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e) by dividing each pixel value by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "X_train, X_test = X_train / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the MNIST dataset contains 70,000 images of size 28 x 28. However, this is a three dimensional input whereas Logistic Regression requires a two dimensional input! We need to vectorize our data by reshaping our data from three dimensions to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Regression with the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Logistic Regression Model on the MNIST Dataset\n",
    "As we did in Part 1, we'll now use `scikit-learn` to implement a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a LogisticRegression object\n",
    "logistic_model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "# Fit the logistic regression algorithm with the training data\n",
    "logistic_model.fit(X_train[:10000, :], Y_train[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it holds up -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "print(\"Logistic Regression Regression accuracy:\", str(logistic_model.score(X_test, Y_test) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While logistic regression achieved ~95% accuracy on the digits dataset, it only achieves ~90% on the MNIST dataset.\n",
    "But what if we need better than this? This is where neural networks come in handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Our First Neural Network\n",
    "## Reloading the MNIST data\n",
    "Unlike scikit-learn's Logistic Regression model, Tensorflow's neural networks can process 3D data. Let's load the MNIST dataset back as a 3D array so we can use it for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() \n",
    "X_train, X_test = X_train / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Neural Network Model\n",
    "\n",
    "The code below creates four layers that we'll be using for our neural network. Follow the docstrings above each layer's declaration to get a sense of what the layer is used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "'''\n",
    "    Vectorize the input for faster processing. Vectorizing an input essentially \n",
    "    means flattening the input's dimensions. In this case, we flatten the input \n",
    "    from a 28 x 28 two-dimensional matrix to a 784 item one-dimensional array.\n",
    "'''\n",
    "flatten_layer = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "\n",
    "\n",
    "'''\n",
    "    This line adds a dense layer to the neural network. Dense layers are fully \n",
    "    connected layers, which are the standard \"layers\" in a neural network.\n",
    "\n",
    "    128 - units of output dimensionality. We generally try to use powers of 2 \n",
    "    (64, 128, 256, etc) here because they're most efficient on GPUs. Finding a \n",
    "    good value here is important - 2048 would be overkill on the MNIST dataset, \n",
    "    but 16 might not be enough.\n",
    "\n",
    "    activation='relu' - relu stands for Rectified Linear Unit. Essentially, this \n",
    "    activation adds non-linearity to the neural network. If you try to run a \n",
    "    linear regression model on this dataset, you'll see it does very poorly. \n",
    "    This suggests that it would be a good idea to add some non-linearity to this \n",
    "    problem.\n",
    "'''\n",
    "dense_relu_layer = tf.keras.layers.Dense(128, activation='relu')\n",
    "\n",
    "\n",
    "'''\n",
    "    Dropout is a good layer for avoiding overfitting - training a machine \n",
    "    learning algorithm on a training set too much. This causes the machine \n",
    "    learning algorithm to notice irrelevant aspects (\"noise\") of the training \n",
    "    set. It essentially adds a layer of randomness to the neural network by \n",
    "    ignoring a percentage of random inputs (in this case, ignore a random 20%) \n",
    "    on each iteration. \n",
    "\n",
    "    \"If you're good at something while drunk, you'll be really good at it sober\" \n",
    "    - Ryan McCormick, former HackBU co-director\n",
    "'''\n",
    "dropout_layer = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "\n",
    "'''\n",
    "    Add another dense layer, but this time with an output dimensionality of 10 \n",
    "    units because there are only 10 options (there are only 10 digits).\n",
    "\n",
    "    activation='softmax' turns the arbitrary outputs of the neural network into \n",
    "    \"probabilities\".\n",
    "\n",
    "    The final decision made by the neural network should be the output with the \n",
    "    highest probability.\n",
    "\n",
    "    Example:\n",
    "        Input: Image of handwritten \"2\"\n",
    "        Output: [0.01, 0.02, 0.9, 0.01, 0.005, 0.005, 0.02, 0.01, 0.01, 0.01]\n",
    "            > Softmax gives us 10 probabilities, one for each digit 0 - 9.\n",
    "        Interpretation: This image is a handwritten \"2\" with probability of 0.9, \n",
    "        or 90%.\n",
    "'''\n",
    "dense_softmax_layer = tf.keras.layers.Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "model = tf.keras.models.Sequential([\n",
    "    flatten_layer,\n",
    "    dense_relu_layer,\n",
    "    dropout_layer,\n",
    "    dense_softmax_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compile our model, which builds our neural network's architecture. Once we've compiled our neural network, we will be able to train it by fitting it to our training set.\n",
    "\n",
    "We compile our model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and fit it to our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "model.fit(X_train, Y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`epochs` is the number of times we fit our neural network to the training set. This value needs to be carefully decided in a neural network. \n",
    "\n",
    "While it may be tempting to make `epochs` as high as possible, too many epochs will cause our neural network to make irrelevant connections, decreasing its accuracy. This is known as **overfitting**.\n",
    "\n",
    "Think of it as that one friend who reads too much into one or two events and does something stupid as a result. *Don't let your neural networks be that friend.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "# Evaluate the accuracy of the neural network and print it out\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our simple neural network greatly outperforms our logistic regression model (~98% vs ~90%). Although an 8% increase may seem somewhat underwhelming, the accuracy increase vs. difficulty scale of neural networks is very much logarithmic. That is, it's often easier to go from 60% to 70% accuracy than it is to go from 98 to 99%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. After we load the MNIST dataset, we implement unit vector scaling by dividing each pixel value by 255. Try loading in the MNIST dataset to your own training/testing variables, and **without scaling the pixel values**, implement the same tensorflow model we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next section (recommended): [House Price Prediction with Machine Learning](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/housing_price_prediction.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
